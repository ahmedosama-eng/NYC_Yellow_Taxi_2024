{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Taxi_analysis_2024\") \\\n",
    "    .config(\"spark.ui.port\", \"4041\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.jars\", r\"C:\\Program Files\\PostgreSQL\\17\\jdbc\\postgresql-42.7.5.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.hadoop.io.native.lib.available\", \"false\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().get(\"spark.jars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = r\"F:\\data project\\Taxi_track_2024\\data\\2024\"\n",
    "data={}\n",
    "for root, _, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_name=file_path.split(\"\\\\\")[5].split(\"_\")[2].split(\".\")[0]\n",
    "        data[file_name]=spark.read.parquet(file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups={}\n",
    "directory = r\"F:\\data project\\Taxi_track_2024\\data\\lookups\"\n",
    "for root, _, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_name=file_path.split(\"\\\\\")[5].split(\".\")[0]\n",
    "        lookups[file_name]=spark.read.csv(file_path,header=True,inferSchema=True)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups['payment_type_lookup'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups['rate_lookup'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups['taxi_zone_lookup'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups['vendor_lookup'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups['Store_and_fwd_flag'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, year, month, dayofmonth, hour, minute, when, date_format,unix_timestamp,count,round,concat_ws,lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_pickup = lookups['taxi_zone_lookup'].alias(\"pickup_zone\")\n",
    "taxi_zone_dropoff = lookups['taxi_zone_lookup'].alias(\"dropoff_zone\")\n",
    "c=0\n",
    "for i in data.keys():\n",
    "        df=data[i].join(lookups['rate_lookup'],lookups['rate_lookup'].rate_id==data[i].RatecodeID,'left')\\\n",
    "                        .join(lookups['payment_type_lookup'],lookups['payment_type_lookup'].payment_id==data[i].payment_type,'left')\\\n",
    "                        .join(taxi_zone_pickup,col('PULocationID')==col(\"pickup_zone.LocationID\"),'left')\\\n",
    "                        .join(taxi_zone_dropoff,col('DOLocationID')==col(\"dropoff_zone.LocationID\"),'left')\\\n",
    "                        .join(lookups['vendor_lookup'],lookups['vendor_lookup'].vendor_id==data[i].VendorID,'left')\\\n",
    "                        .join(lookups['Store_and_fwd_flag'],lookups['Store_and_fwd_flag'].flag==data[i].store_and_fwd_flag,'left')\\\n",
    "                        .withColumn('tpep_pickup_datetime',col('tpep_pickup_datetime').cast('timestamp'))\\\n",
    "                        .withColumn('tpep_dropoff_datetime',col('tpep_dropoff_datetime').cast('timestamp'))\\\n",
    "                        .withColumn(\"duration\",round((unix_timestamp(col(\"tpep_dropoff_datetime\")) - unix_timestamp(col(\"tpep_pickup_datetime\"))) / 60,2))\\\n",
    "                        .withColumn(\"pickup_month\", month(\"tpep_pickup_datetime\")) \\\n",
    "                        .withColumn(\"pickup_day\", dayofmonth(\"tpep_pickup_datetime\")) \\\n",
    "                        .withColumn(\"pickup_day_of_week\", date_format(\"tpep_pickup_datetime\", \"EEEE\"))\\\n",
    "                        .withColumn(\"pickup_hour\", hour(\"tpep_pickup_datetime\")) \\\n",
    "                        .withColumn(\"pickup_minute\", minute(\"tpep_pickup_datetime\")) \\\n",
    "                        .withColumn(\"pickup_time_of_day\", \n",
    "                                when(col(\"pickup_hour\").between(0, 5), \"Morning\")\n",
    "                                .when(col(\"pickup_hour\").between(6, 11), \"Afternoon\")\n",
    "                                .when(col(\"pickup_hour\").between(12, 17), \"Evening\")\n",
    "                                .when(col(\"pickup_hour\").between(18, 23), \"Night\")\n",
    "                                )\\\n",
    "                        .withColumn(\"dropoff_month\", month(\"tpep_dropoff_datetime\")) \\\n",
    "                        .withColumn(\"dropoff_day\", dayofmonth(\"tpep_dropoff_datetime\")) \\\n",
    "                        .withColumn(\"dropoff_day_of_week\", date_format(\"tpep_dropoff_datetime\", \"EEEE\"))\\\n",
    "                        .withColumn(\"dropoff_hour\", hour(\"tpep_dropoff_datetime\")) \\\n",
    "                        .withColumn(\"dropoff_minute\", minute(\"tpep_dropoff_datetime\")) \\\n",
    "                        .withColumn(\"dropoff_time_of_day\", \n",
    "                                when(col(\"dropoff_hour\").between(0, 5), \"Morning\")\n",
    "                                .when(col(\"dropoff_hour\").between(6, 11), \"Afternoon\")\n",
    "                                .when(col(\"dropoff_hour\").between(12, 17), \"Evening\")\n",
    "                                .when(col(\"dropoff_hour\").between(18, 23), \"Night\")\n",
    "                                )\n",
    "        if c==0:\n",
    "                df_marged=df\n",
    "                c=1\n",
    "                continue\n",
    "        \n",
    "        df_marged=df_marged.union(df)\n",
    "\n",
    "\n",
    "df_marged=df_marged.select(\n",
    "                col('vendor_name'),\n",
    "                col('flag_name'),\n",
    "                col('pickup_zone.Borough').alias(\"PickupBorough\"),\n",
    "                col('pickup_zone.Zone').alias(\"PickupZone\"),\n",
    "                col('pickup_zone.service_zone').alias(\"Pickupservice_zone\"),\n",
    "                col('dropoff_zone.Borough').alias(\"DropoffBorough\"),\n",
    "                col('dropoff_zone.Zone').alias(\"DropoffZone\"),\n",
    "                col('dropoff_zone.service_zone').alias(\"Dropoffservice_zone\"),\n",
    "                col('payment_name'),\n",
    "                col('rate_name'),\n",
    "                col('tpep_pickup_datetime'),\n",
    "                col('pickup_month'),\n",
    "                col('pickup_day'),\n",
    "                col('pickup_day_of_week'),\n",
    "                col('pickup_hour'),\n",
    "                col('pickup_minute'),\n",
    "                col('pickup_time_of_day'),\n",
    "                col('tpep_dropoff_datetime'),\n",
    "                col('dropoff_month'),\n",
    "                col('dropoff_day'),\n",
    "                col('dropoff_day_of_week'),\n",
    "                col('dropoff_hour'),\n",
    "                col('dropoff_minute'),\n",
    "                col('dropoff_time_of_day'),\n",
    "                col('duration'),\n",
    "                col('passenger_count'),\n",
    "                col('trip_distance'),\n",
    "                col('fare_amount'),\n",
    "                col('extra'),\n",
    "                col('mta_tax'),\n",
    "                col('tip_amount'),\n",
    "                col('improvement_surcharge'),\n",
    "                col('total_amount'),\n",
    "                col('congestion_surcharge'),\n",
    "                col('Airport_fee')\n",
    "            )\n",
    "\n",
    "df_marged=df_marged.fillna({\"passenger_count\": 0})\\\n",
    "                   .fillna({\"congestion_surcharge\": 0})\\\n",
    "                   .fillna({\"Airport_fee\": 0})\\\n",
    "                   .fillna({\"flag_name\": \"unknown\"})\\\n",
    "                   .fillna({\"payment_name\":  \"unknown\"})\\\n",
    "                   .fillna({\"rate_name\": \"unknown\"})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marged.groupBy(\"store_and_fwd_flag\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marged.groupBy(\"flag_name\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marged.groupBy(\"rate_id\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marged.groupBy(\"payment_id\").count().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marged.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, avg, count, max, min,format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def aggregate_Taxi(df, groupby_cols):\n",
    "    grouped_df = df.groupBy([F.col(col) for col in groupby_cols])\n",
    "    agg_exprs = [\n",
    "        F.format_number(F.sum('trip_distance'),2).alias('sum_trip_distance_in_mile'),\n",
    "        F.format_number(F.avg('trip_distance'), 2).alias('avg_trip_distance_in_mile'),\n",
    "        F.format_number(F.max('trip_distance'), 2).alias('max_trip_distance_in_mile'),\n",
    "        F.format_number(F.min('trip_distance'), 2).alias('min_trip_distance_in_mile'),\n",
    "        F.format_number(F.sum('duration'),2).alias('sum_trip_duration'),\n",
    "        F.format_number(F.avg('duration'),2).alias('avg_trip_duration'),\n",
    "        F.format_number(F.max('duration'),2).alias('max_trip_duration'),\n",
    "        F.format_number(F.min('duration'),2).alias('min_trip_duration'),\n",
    "        F.format_number(F.sum('fare_amount'), 2).alias('sum_fare_amount'),\n",
    "        F.format_number(F.avg('fare_amount'), 2).alias('avg_fare_amount'),\n",
    "        F.format_number(F.max('fare_amount'), 2).alias('max_fare_amount'),\n",
    "        F.format_number(F.min('fare_amount'), 2).alias('min_fare_amount'),\n",
    "        F.format_number(F.sum('extra'), 2).alias('sum_extra'),\n",
    "        F.format_number(F.avg('extra'), 2).alias('avg_extra'),\n",
    "        F.format_number(F.max('extra'), 2).alias('max_extra'),\n",
    "        F.format_number(F.min('extra'), 2).alias('min_extra'),\n",
    "        F.format_number(F.sum('mta_tax'), 2).alias('sum_mta_tax'),\n",
    "        F.format_number(F.avg('mta_tax'), 2).alias('avg_mta_tax'),\n",
    "        F.format_number(F.max('mta_tax'), 2).alias('max_mta_tax'),\n",
    "        F.format_number(F.min('mta_tax'), 2).alias('min_mta_tax'),\n",
    "        F.format_number(F.sum('tip_amount'), 2).alias('sum_tip_amount'),\n",
    "        F.format_number(F.avg('tip_amount'), 2).alias('avg_tip_amount'),\n",
    "        F.format_number(F.max('tip_amount'), 2).alias('max_tip_amount'),\n",
    "        F.format_number(F.min('tip_amount'), 2).alias('min_tip_amount'),\n",
    "        F.format_number(F.sum('improvement_surcharge'), 2).alias('sum_improvement_surcharge'),\n",
    "        F.format_number(F.avg('improvement_surcharge'), 2).alias('avg_improvement_surcharge'),\n",
    "        F.format_number(F.max('improvement_surcharge'), 2).alias('max_improvement_surcharge'),\n",
    "        F.format_number(F.min('improvement_surcharge'), 2).alias('min_improvement_surcharge'),\n",
    "        F.format_number(F.sum('total_amount'), 2).alias('sum_total_amount'),\n",
    "        F.format_number(F.avg('total_amount'), 2).alias('avg_total_amount'),\n",
    "        F.format_number(F.max('total_amount'), 2).alias('max_total_amount'),\n",
    "        F.format_number(F.min('total_amount'), 2).alias('min_total_amount'),\n",
    "        F.format_number(F.sum('congestion_surcharge'), 2).alias('sum_congestion_surcharge'),\n",
    "        F.format_number(F.avg('congestion_surcharge'), 2).alias('avg_congestion_surcharge'),\n",
    "        F.format_number(F.max('congestion_surcharge'), 2).alias('max_congestion_surcharge'),\n",
    "        F.format_number(F.min('congestion_surcharge'), 2).alias('min_congestion_surcharge'),\n",
    "        F.format_number(F.sum('Airport_fee'), 2).alias('sum_Airport_fee'),\n",
    "        F.format_number(F.avg('Airport_fee'), 2).alias('avg_Airport_fee'),\n",
    "        F.format_number(F.max('Airport_fee'), 2).alias('max_Airport_fee'),\n",
    "        F.format_number(F.min('Airport_fee'), 2).alias('min_Airport_fee'),\n",
    "        F.count('Airport_fee').alias('Trip_counts')\n",
    "    ]\n",
    "    result_df = grouped_df.agg(*agg_exprs)\n",
    "    return result_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "groupby_cols=['vendor_name',\n",
    " 'flag_name',\n",
    " 'payment_name',\n",
    " 'rate_name',\n",
    " 'PickupBorough',\n",
    " 'PickupZone',\n",
    " 'Pickupservice_zone',\n",
    " 'pickup_month',\n",
    " 'pickup_day',\n",
    " 'pickup_day_of_week',\n",
    " 'pickup_time_of_day',\n",
    " 'DropoffBorough',\n",
    " 'DropoffZone',\n",
    " 'Dropoffservice_zone',\n",
    " 'dropoff_month',\n",
    " 'dropoff_day',\n",
    " 'dropoff_day_of_week',\n",
    " 'dropoff_time_of_day']\n",
    "a=aggregate_Taxi(df_marged,groupby_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=[\n",
    " 'vendor_name',\n",
    " 'flag_name',\n",
    " 'payment_name',\n",
    " 'rate_name'\n",
    "]\n",
    "df2=[\n",
    " 'DropoffZone',\n",
    " 'dropoff_month'\n",
    "]\n",
    "\n",
    "df3=[\n",
    " 'PickupZone',\n",
    " 'pickup_month'\n",
    "]\n",
    "\n",
    "df4=['PickupZone',\n",
    "    'DropoffZone']\n",
    "\n",
    "df5=['PickupZone',\n",
    "    'pickup_day_of_week']\n",
    "\n",
    "df6=['DropoffZone',\n",
    "    'pickup_day_of_week']\n",
    "\n",
    "\n",
    "df7=['pickup_month',\n",
    "    'pickup_day_of_week']\n",
    "\n",
    "\n",
    "insight = {\n",
    "    'Vendor, flag type, rate, and payment type analysis': df1,\n",
    "    'Dropoff location and month analysis': df2,\n",
    "    'Pickup location and month analysis': df3,\n",
    "    'Pickup and dropoff zone relationship analysis': df4,\n",
    "    'Pickup location and day of the week analysis': df5,\n",
    "    'Dropoff location and pickup day of the week analysis': df6,\n",
    "    'Pickup month and day of the week trends': df7,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregatation_piovte_table={}\n",
    "for description,dimention in insight.items() :\n",
    "    pivote_table=aggregate_Taxi(df_marged,dimention)\n",
    "    aggregatation_piovte_table[description]=pivote_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_exists(index_path):\n",
    "    return os.path.exists(index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "from LLM_integration.data_ingestion import prepare_for_embadding\n",
    "from utils import check_file_exists\n",
    "\n",
    "\n",
    "def documents_converter(data_dict):\n",
    "    print(\"here we convert to doc\")\n",
    "    documents = []\n",
    "    \n",
    "    for filename, df in data_dict.items():\n",
    "            print(filename)\n",
    "            text_rows = []\n",
    "            for row in df.toLocalIterator():  \n",
    "                text_rows.append(\", \".join(map(str, row)))\n",
    "                if len(text_rows) >= 1000:  \n",
    "                    documents.append(Document(page_content=\"\\n\".join(text_rows), metadata={\"source\": filename}))\n",
    "                    text_rows = []  \n",
    "            \n",
    "            \n",
    "            if text_rows:\n",
    "                documents.append(Document(page_content=\"\\n\".join(text_rows), metadata={\"source\": filename}))\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "def text_spliter(data):           \n",
    "    print(\"here we splite to chunks\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "    text_chunks = text_splitter.split_documents(data)\n",
    "    return text_chunks\n",
    "\n",
    "\n",
    "def Embeddings_and_vectordb(data,vdb_path,cheak_path):\n",
    "    # cheak of its already embadded\n",
    "    if check_file_exists(cheak_path)==True:\n",
    "        docsearch = FAISS.load_local(cheak_path,embeddings=OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\")),allow_dangerous_deserialization=True)\n",
    "    else :\n",
    "        data_dictionary=prepare_for_embadding(data)\n",
    "        documents=documents_converter(data_dictionary)\n",
    "        chunks=text_spliter(documents)\n",
    "        print(\"here we embedding\")\n",
    "        embedding = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        docsearch = FAISS.from_documents(chunks, embedding)\n",
    "        docsearch.save_local(vdb_path)\n",
    "    return docsearch\n",
    "\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
